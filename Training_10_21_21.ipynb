{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8272a45",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2b86b",
   "metadata": {},
   "source": [
    "##### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66de626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from typing import List\n",
    "\n",
    "## Set up logger to get details of errors\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1a9ea",
   "metadata": {},
   "source": [
    "##### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55ca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df_binary = pd.read_csv(\"https://raw.githubusercontent.com/jcox22/Sagemaker_practice_gan/main/rank_1_curves.csv\")\n",
    "coef_df_binary = coef_df_binary.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fb104",
   "metadata": {},
   "source": [
    "##### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5f0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(coef_df_binary.to_numpy())\n",
    "train_ds = torch.utils.data.TensorDataset(train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9993b9c",
   "metadata": {},
   "source": [
    "##### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffa594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is for number of nodes in each hidden layer of NN\n",
    "k = 1000\n",
    "\n",
    "# For number of inputs (32 binary digits)\n",
    "input_length = 32\n",
    "output_length = input_length\n",
    "\n",
    "# Model Parameters\n",
    "epochs = 1\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# Needed later on for save_model\n",
    "model_dir = '/models'\n",
    "data_dir = '/training'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9093c",
   "metadata": {},
   "source": [
    "##### Create NN Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711594c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_length: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(output_length, k)\n",
    "        self.dense_layer2 = nn.Linear(k, k)\n",
    "        self.dense_layer3 = nn.Linear(k, k)\n",
    "        self.dense_layer4 = nn.Linear(k, output_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.dense_layer(x)\n",
    "        l2 = self.dense_layer2(F.relu(l1))\n",
    "        l3 = self.dense_layer3(F.relu(l2))\n",
    "        l4 = self.dense_layer4(F.relu(l3))\n",
    "        return F.sigmoid(l4)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(int(input_length), k)\n",
    "        self.dense_layer2 = nn.Linear(k, k)\n",
    "        self.dense_layer3 = nn.Linear(k, k)\n",
    "        self.dense_layer4 = nn.Linear(k, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.dense_layer(x)\n",
    "        l2 = self.dense_layer2(F.relu(l1))\n",
    "        l3 = self.dense_layer3(F.relu(l2))\n",
    "        l4 = self.dense_layer4(F.relu(l3))\n",
    "        return F.sigmoid(l4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e72c88",
   "metadata": {},
   "source": [
    "##### Set up for training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdfc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store on GPU else cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(\"Device Type: {}\".format(device))\n",
    "\n",
    "# Call generator and discriminator\n",
    "generator = Generator(output_length)\n",
    "discriminator = Discriminator(input_length)\n",
    "\n",
    "# Make sure it is on device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Loss\n",
    "loss = nn.BCELoss().to(device)\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8e06",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44309d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        noise = torch.randint(0, 2, size=(batch_size, output_length)).float()\n",
    "        noise = noise.to(device)\n",
    "    \n",
    "        # Generate examples of data\n",
    "        true_labels = [1] * batch_size\n",
    "        true_labels = torch.tensor(true_labels).float()\n",
    "        true_labels = true_labels.to(device).resize_((batch_size, 1))\n",
    "            \n",
    "        true_data = coef_df_binary.sample(batch_size).values\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "        true_data = true_data.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        #outputs = model(inputs)\n",
    "        #G_of_noise = generator(noise)\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        G_of_noise = generator(noise)\n",
    "        D_of_G_of_noise = discriminator(G_of_noise)\n",
    "        generator_loss = loss(D_of_G_of_noise, true_labels)\n",
    "        generator_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Train the discriminator on the true/generated data\n",
    "        optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(G_of_noise.detach()) # introduce new d_of_g_of_noise without gradient\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size).to(device).resize_((batch_size, 1)))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += generator_loss.item()\n",
    "        #if batch % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            #print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            #print(f\"Loss is {generator_loss.item()}.  Running loss is {running_loss/2000}.  Discriminator loss is {discriminator_loss.item()}.  True Discriminator Loss is {true_discriminator_loss.item()}\")\n",
    "            #print(GPUtil.showUtilization())\n",
    "            #running_loss = 0.0\n",
    "            #print(torch.cuda.memory_summary(device))\n",
    "            #print(torch.cuda.list_gpu_processes(device))\n",
    "        # print(running_loss/10)\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78567d2",
   "metadata": {},
   "source": [
    "##### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f13c7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c3567550f313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m'generator_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     }, output_model)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output_model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cb498",
   "metadata": {},
   "source": [
    "##### Function to change binary to decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91347870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(G_of_noise):\n",
    "\n",
    "    G_numpy = G_of_noise.detach()   \n",
    "\n",
    "    curves = []\n",
    "\n",
    "    for i in range(len(G_numpy)):\n",
    "        c1 = int(G_numpy[i][0].round())\n",
    "        c2 = ((-1)**(int(G_numpy[i][1].round())))*(int(G_numpy[i][2].round()))\n",
    "        c3 = int(G_numpy[i][3].round())\n",
    "        c4 = (-1)**(int(G_numpy[i][4].round()))*(int(\"\".join([str(int(y)) for y in G_numpy[i][5:18].round()]), 2))\n",
    "        c6 = (-1)**(int(G_numpy[i][12].round()))*(int(\"\".join([str(int(y)) for y in G_numpy[i][19:].round()]), 2))\n",
    "    \n",
    "        coef = [c1,c2,c3,c4,c6]\n",
    "        curves.append(coef)\n",
    "    return curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
