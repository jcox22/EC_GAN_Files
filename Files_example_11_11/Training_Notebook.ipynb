{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8272a45",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2b86b",
   "metadata": {},
   "source": [
    "##### Import Packages and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "66de626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Set up logger to get details of errors\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "## Set up device settings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1a9ea",
   "metadata": {},
   "source": [
    "##### Import Data\n",
    "\n",
    "The imported data is a list of elliptic curves of rank 1.  Each curve is represented in binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b55ca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df_binary = pd.read_csv(\"https://raw.githubusercontent.com/jcox22/Sagemaker_practice_gan/main/rank_1_curves.csv\")\n",
    "coef_df_binary = coef_df_binary.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba10bdc-9d00-470b-b42e-c6056dc8fb3f",
   "metadata": {},
   "source": [
    "One of the key values we use in the training of our model is the variance of the original dataset.  The idea is that we can try to mimick the natural variance in that dataset to create heterogeneity in the curves we generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a2d31750-8b81-4536-b7f3-d7d5908d2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variance variables from data\n",
    "variances = coef_df_binary.describe().loc[['std']]**2\n",
    "real_var = torch.tensor(variances.to_numpy(), dtype = torch.float32, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9993b9c",
   "metadata": {},
   "source": [
    "##### Set Parameters\n",
    "\n",
    "The following are all key parameters of the GAN model we are training.  We also create names for the specific model we are training by the values of these parameters.  That way when we are looking back at the long list of models we test, it is easy to identify how each model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bffa594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is for number of nodes in each hidden layer of NN\n",
    "k = 1000\n",
    "\n",
    "# For number of inputs (32 binary digits)\n",
    "input_length = 32\n",
    "output_length = input_length\n",
    "\n",
    "# Model Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "var_weight = 0.1\n",
    "\n",
    "# Needed later on for save_model\n",
    "model_dir = '/models'\n",
    "data_dir = '/training'\n",
    "model_name = '4layers_' + str(epochs) + 'epochs_' + str(k) + 'nodes_' + str(batch_size) + 'batch_size_' + str(lr) + 'lr_' + str(var_weight) + 'var_weight'\n",
    "loss_folder = './Losses/' + model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b2308-800c-4aaa-b38a-b7fc73fbaa9b",
   "metadata": {},
   "source": [
    "##### Create New Folders to Save Loss Info\n",
    "\n",
    "One of the statistics that we want to track is loss information over each epoch.  To simplify the process over training many models, we can save each models loss infomation in its own folder.  These folders are found under the 'Losses' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cc523c0c-3120-47d8-b644-e7dd9c2b2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory ./Losses/4layers_100epochs_1000nodes_256batch_size_0.001lr_0.1var_weight failed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(loss_folder)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % loss_folder)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % loss_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fb104",
   "metadata": {},
   "source": [
    "##### Training Dataset\n",
    "\n",
    "The following cell sets up the training data that we previously loaded into the format that Pytorch prefers for its training jobs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3a5f0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(coef_df_binary.to_numpy(), dtype = torch.float32, device = device)\n",
    "train_ds = torch.utils.data.TensorDataset(train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6604e3-7dbd-4c65-9d0a-609afff3e9cd",
   "metadata": {},
   "source": [
    "##### Set Distribution of initial inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8bd77c5e-c75b-44fa-8c9e-d64379cf7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9093c",
   "metadata": {},
   "source": [
    "##### Create NN Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "711594c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_length: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(output_length, k)\n",
    "        self.dense_layer2 = nn.Linear(k, k)\n",
    "        self.dense_layer3 = nn.Linear(k, k)\n",
    "        self.dense_layer4 = nn.Linear(k, output_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.dense_layer(x)\n",
    "        l2 = self.dense_layer2(F.relu(l1))\n",
    "        l3 = self.dense_layer3(F.relu(l2))\n",
    "        l4 = self.dense_layer4(F.relu(l3))\n",
    "        return l4\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(int(input_length), k)\n",
    "        self.dense_layer2 = nn.Linear(k, k)\n",
    "        self.dense_layer3 = nn.Linear(k, k)\n",
    "        self.dense_layer4 = nn.Linear(k, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.dense_layer(x)\n",
    "        l2 = self.dense_layer2(F.relu(l1))\n",
    "        l3 = self.dense_layer3(F.relu(l2))\n",
    "        l4 = self.dense_layer4(F.relu(l3))\n",
    "        return l4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e72c88",
   "metadata": {},
   "source": [
    "##### Set up for training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "abdfc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2065128/3949670600.py:3: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(m.weight)\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Device Type: {}\".format(device))\n",
    "\n",
    "# Call generator and discriminator\n",
    "generator = Generator(output_length)\n",
    "discriminator = Discriminator(input_length)\n",
    "\n",
    "# Make sure it is on device\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Apply distrubution type\n",
    "generator.apply(init_normal)\n",
    "discriminator.apply(init_normal)\n",
    "\n",
    "# Loss\n",
    "loss = nn.BCEWithLogitsLoss().to(device)\n",
    "MSE = torch.nn.MSELoss(reduction = 'sum').to(device)\n",
    "\n",
    "# Choose optimizer\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=momentum)\n",
    "\n",
    "dis_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8e06",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "44309d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/jcox22/anaconda3/envs/pytorch110/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/jcox22/anaconda3/envs/pytorch110/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "running_loss_list = []\n",
    "average_td_gd_loss_list = []\n",
    "running_loss_true_d_list = []\n",
    "g_d_running_loss_list = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    running_loss = 0.0\n",
    "    average_td_gd_loss = 0.0\n",
    "    running_loss_true_d = 0.0\n",
    "    g_d_running_loss = 0.0\n",
    "    \n",
    "    for batch in enumerate(train_loader):\n",
    "        noise = torch.randint(0, 2, size=(batch_size, output_length)).float()\n",
    "        noise = noise.to(device)\n",
    "    \n",
    "        # Generate examples of data\n",
    "        true_labels = [1] * batch_size\n",
    "        true_labels = torch.tensor(true_labels).float()\n",
    "        true_labels = true_labels.to(device).resize_((batch_size, 1))\n",
    "            \n",
    "        true_data = batch[1][0]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        #outputs = model(inputs)\n",
    "        #G_of_noise = generator(noise)\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        G_of_noise = generator(noise)\n",
    "        D_of_G_of_noise = discriminator(G_of_noise)\n",
    "        generator_loss = loss(D_of_G_of_noise, true_labels) + MSE(real_var, torch.var(G_of_noise, dim = 0))*var_weight\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "            \n",
    "        # Train the discriminator on the true/generated data\n",
    "        dis_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(G_of_noise.detach()) # introduce new d_of_g_of_noise without gradient\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size).to(device).resize_((batch_size, 1)))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += generator_loss.item() * len(batch)\n",
    "        average_td_gd_loss += discriminator_loss.item() * len(batch)\n",
    "        running_loss_true_d += true_discriminator_loss.item() * len(batch)\n",
    "        g_d_running_loss += generator_discriminator_loss.item() * len(batch)\n",
    "\n",
    "    #print(f\"epoch:{epoch} average generator loss {running_loss / len(train_loader.dataset)}\")\n",
    "    #print(f\"epoch:{epoch} average discriminator loss {average_td_gd_loss / len(train_loader.dataset)}\")\n",
    "    #print(f\"epoch:{epoch} average true discriminator loss {running_loss_true_d / len(train_loader.dataset)}\")\n",
    "    #print(f\"epoch:{epoch} average generator discriminator loss {g_d_running_loss / len(train_loader.dataset)}\")\n",
    "    running_loss_list.append(running_loss / len(train_loader.dataset))\n",
    "    average_td_gd_loss_list.append(average_td_gd_loss / len(train_loader.dataset))\n",
    "    running_loss_true_d_list.append(running_loss_true_d / len(train_loader.dataset))\n",
    "    g_d_running_loss_list.append(g_d_running_loss / len(train_loader.dataset))\n",
    "\n",
    "    print(\"Finished Epoch\")\n",
    "   \n",
    "## Save Losses\n",
    "with open(loss_folder + '/running_loss.txt', \"w\") as output:\n",
    "    output.write(str(running_loss_list))\n",
    "    \n",
    "with open(loss_folder + '/average_td_gd_loss.txt', \"w\") as output:\n",
    "    output.write(str(average_td_gd_loss_list))\n",
    "    \n",
    "with open(loss_folder + '/running_loss_true_d.txt', \"w\") as output:\n",
    "    output.write(str(running_loss_true_d_list))\n",
    "    \n",
    "with open(loss_folder + '/g_d_running_loss.txt', \"w\") as output:\n",
    "    output.write(str(g_d_running_loss_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78567d2",
   "metadata": {},
   "source": [
    "##### Saving the Model\n",
    "\n",
    "The trained model will now be available in the 'Trained_Models' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "32f13c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './Trained_Models/generator_' + model_name + '.pt'\n",
    "\n",
    "torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'optimizer_state_dict': gen_optimizer.state_dict()\n",
    "    }, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9115ae-ba7e-485f-bb09-cc488e1dab23",
   "metadata": {},
   "source": [
    "##### Now we have a saved model in the Trained_Models folder\n",
    "\n",
    "Head over to the 'Model_Evaluation' notebook to see what kinds of curves this model generates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
